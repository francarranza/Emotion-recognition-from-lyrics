{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 445 ms, sys: 48.6 ms, total: 494 ms\n",
      "Wall time: 632 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "import bisect\n",
    "import codecs\n",
    "import nltk\n",
    "from aux_bisect import *\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEXICONS_DIR = 'datasets/lexicons/'\n",
    "MOODY_DIR = 'datasets/moody_lyrics/raw_lyrics/'\n",
    "BILBOARD = 'datasets/lyrics_bilboard.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargamos cada lyric en una lista\n",
    "def load_moody():\n",
    "    lyrics_list = []\n",
    "    \n",
    "    for filename in os.listdir(MOODY_DIR):\n",
    "        with codecs.open(MOODY_DIR+filename, 'r', 'utf-8') as f:\n",
    "            lyrics_list.append(f.read())\n",
    "    return lyrics_list\n",
    "\n",
    "\n",
    "def load_bilboard():\n",
    "    lyrics_list = []\n",
    "    with codecs.open(BILBOARD, 'r', 'utf-8') as f:\n",
    "        lyrics_list = f.read().splitlines()\n",
    "    return lyrics_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = 'moody'\n",
    "\n",
    "if option == 'moody':\n",
    "    lyrics_list = load_moody()\n",
    "    \n",
    "elif option == 'bilboard':\n",
    "    lyrics_list = load_bilboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyric example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this world of ordinary people\n",
      "Extraordinary people\n",
      "I'm glad there is you\n",
      "\n",
      "In this world of over-rated treasures\n",
      "Of under-rated pleasures\n",
      "I'm so glad there is you\n",
      "\n",
      "I live to love\n",
      "I love to live with you be, beside me\n",
      "This role so new\n",
      "I'll muddle through, with you to guide me\n",
      "\n",
      "So in this world where many, many play at love\n",
      "And hardly any stay in love\n",
      "I'm glad there is you more than ever\n",
      "I'm glad there is you\n",
      "\n",
      "I live to love\n",
      "I love to live with you beside me\n",
      "This role so new\n",
      "I'll muddle through with you to guide me\n",
      "\n",
      "So in this world, in this world, in this world\n",
      "Where many, many play at love\n",
      "But hardly any stay in love\n",
      "I'm glad there is you more than ever\n",
      "I'm glad there is you\n"
     ]
    }
   ],
   "source": [
    "print(lyrics_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length ANEW Lexicon:  1030\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'datasets/lexicons/general_inquirer.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-75c062ad86ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Pre-process General Inquirer Sentiment Lexicons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgeneral_inquirer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEXICONS_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'general_inquirer.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msenti\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgeneral_inquirer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesis/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesis/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesis/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesis/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesis/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'datasets/lexicons/general_inquirer.csv' does not exist"
     ]
    }
   ],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "anew_raw = pd.read_csv(LEXICONS_DIR+'anew.csv')\n",
    "anew_clean = anew_raw[['Description', 'Valence Mean', 'Arousal Mean']].sort_values(by='Description')\n",
    "anew_words = anew_clean['Description'].tolist()\n",
    "print(\"Length ANEW Lexicon: \", len(anew_words))\n",
    "\n",
    "'''\n",
    "# Pre-process General Inquirer Sentiment Lexicons\n",
    "general_inquirer = pd.read_csv(LEXICONS_DIR+'general_inquirer.csv')['A'].tolist()\n",
    "gi = []\n",
    "for senti in general_inquirer:\n",
    "    senti = str(senti.split('#')[0])\n",
    "    if not senti in gi:\n",
    "        gi.append(senti.lower())\n",
    "print(\"Length General Inquirer Lexicon: \", len(gi))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Positive/Negative English Lexicon:  4376\n"
     ]
    }
   ],
   "source": [
    "# Pre proceso de positive/negative english words\n",
    "with open(LEXICONS_DIR+'positive_words_en.txt') as f:\n",
    "    en_pos = f.read().splitlines()\n",
    "\n",
    "with open(LEXICONS_DIR+'negative_words_en.txt') as f:\n",
    "    en_neg = f.read().splitlines()\n",
    "    \n",
    "en_pos_neg = sorted(en_pos + en_neg)\n",
    "print(\"Length Positive/Negative English Lexicon: \", len(en_pos_neg))\n",
    "\n",
    "mixed_corpus_not_stemmed = anew_words+en_pos_neg\n",
    "\n",
    "# Hay que stemmear el corpus\n",
    "p_stemmer = PorterStemmer()\n",
    "mixed_corpus = []\n",
    "for token in mixed_corpus_not_stemmed:\n",
    "    mixed_corpus.append(p_stemmer.stem(token))\n",
    "mixed_corpus = sorted(mixed_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2535"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Process a word\n",
    "def word_processor(word, sentiment, lexicon=anew_words):\n",
    "    word = word.lower()\n",
    "    if not word in STOPWORDS and word.isalpha():\n",
    "        if sentiment:\n",
    "            if find_index(lexicon, word):\n",
    "                return word\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def tokenize_lyric(lyric, sentiment=True):\n",
    "    ## Pre-procesamos el corpus\n",
    "    toktok = ToktokTokenizer()\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    words = toktok.tokenize(lyric)\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        stemmed = p_stemmer.stem(w)\n",
    "        clean_word = word_processor(stemmed, sentiment, mixed_corpus)\n",
    "        if clean_word:\n",
    "            filtered_words.append(clean_word)\n",
    "            \n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "#  Tokenizamos y sacamos las letras con menos de 10 palabras\n",
    "filtered_lyrics = []\n",
    "for lyric in lyrics_list:\n",
    "    tokenized = tokenize_lyric(lyric, True)\n",
    "    if len(tokenized) >= 3:\n",
    "        filtered_lyrics.append(tokenized)\n",
    "len(filtered_lyrics)\n",
    "#print(filtered_lyrics[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionario y BOW corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Corpus Length = 2535\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(filtered_lyrics)\n",
    "corpus_bow = [dictionary.doc2bow(lyric) for lyric in filtered_lyrics]\n",
    "print(\"BOW Corpus Length = \" + str(len(corpus_bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a document-term matrix + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "def apply_lda(corpus, num_topics, passes):\n",
    "    ldamodel = LdaMulticore(\n",
    "        corpus, \n",
    "        num_topics=num_topics, \n",
    "        id2word = dictionary, \n",
    "        passes = passes,\n",
    "        workers = 3,\n",
    "        minimum_probability=0.001\n",
    "    )        \n",
    "    return ldamodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lda(ldamodel, num_words=6):\n",
    "    prin = ldamodel.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "    for i in prin:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.989*\"love\" + 0.082*\"babi\" + 0.057*\"like\" + 0.036*\"prais\" + 0.034*\"heart\"'),\n",
       " (1,\n",
       "  '0.920*\"god\" + 0.202*\"like\" + 0.158*\"power\" + 0.156*\"awesom\" + 0.085*\"home\"'),\n",
       " (2,\n",
       "  '-0.534*\"babi\" + -0.415*\"home\" + -0.388*\"girl\" + -0.272*\"like\" + 0.237*\"god\"'),\n",
       " (3,\n",
       "  '-0.724*\"fire\" + 0.361*\"home\" + -0.304*\"like\" + -0.284*\"burn\" + 0.258*\"babi\"'),\n",
       " (4,\n",
       "  '-0.526*\"christma\" + -0.446*\"merri\" + 0.444*\"girl\" + -0.377*\"home\" + 0.235*\"lone\"'),\n",
       " (5,\n",
       "  '0.465*\"home\" + -0.460*\"christma\" + -0.415*\"merri\" + -0.367*\"girl\" + 0.279*\"fire\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "\n",
    "num_topics = 6\n",
    "lsi = LsiModel(corpus_bow, id2word=dictionary, num_topics=num_topics)\n",
    "lsi.print_topics(num_topics=num_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Process, HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.184*love + 0.029*babi + 0.021*like + 0.017*heart + 0.013*girl'),\n",
       " (1, '0.063*love + 0.019*girl + 0.017*babi + 0.017*joy + 0.014*like'),\n",
       " (2, '0.033*lone + 0.017*girl + 0.008*fire + 0.007*heaven + 0.007*holi'),\n",
       " (3, '0.021*fire + 0.011*love + 0.009*god + 0.007*fight + 0.007*like'),\n",
       " (4, '0.030*home + 0.024*love + 0.021*babi + 0.007*like + 0.006*girl'),\n",
       " (5, '0.023*love + 0.008*babi + 0.007*lie + 0.004*time + 0.004*rock'),\n",
       " (6, '0.019*love + 0.010*happi + 0.007*lie + 0.005*music + 0.005*like'),\n",
       " (7, '0.056*love + 0.005*song + 0.004*time + 0.004*hate + 0.004*life')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp = HdpModel(corpus_bow, id2word=dictionary)\n",
    "hdp.print_topics(num_topics=8, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans con TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster:  0\n",
      "lone\n",
      "long\n",
      "girl\n",
      "time\n",
      "die\n",
      "babi\n",
      "like\n",
      "Cluster:  1\n",
      "love\n",
      "babi\n",
      "like\n",
      "heart\n",
      "life\n",
      "time\n",
      "world\n",
      "Cluster:  2\n",
      "sin\n",
      "hay\n",
      "su\n",
      "mar\n",
      "culpabl\n",
      "error\n",
      "air\n",
      "Cluster:  3\n",
      "home\n",
      "babi\n",
      "long\n",
      "sweet\n",
      "alon\n",
      "christma\n",
      "like\n",
      "Cluster:  4\n",
      "god\n",
      "peac\n",
      "holi\n",
      "heavenli\n",
      "sleep\n",
      "silent\n",
      "heaven\n",
      "Cluster:  5\n",
      "christma\n",
      "merri\n",
      "blue\n",
      "happi\n",
      "white\n",
      "tree\n",
      "love\n",
      "Cluster:  6\n",
      "war\n",
      "fight\n",
      "burn\n",
      "like\n",
      "die\n",
      "death\n",
      "time\n",
      "Cluster:  7\n",
      "love\n",
      "babi\n",
      "girl\n",
      "like\n",
      "time\n",
      "heart\n",
      "life\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_lyric)\n",
    "X = vectorizer.fit_transform(lyrics_list)\n",
    "\n",
    "true_k = 8\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster: \", i)\n",
    "    for ind in order_centroids[i, :7]:\n",
    "        print(terms[ind])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process a word\n",
    "def word_processor2(word, sentiment, lexicon=anew_words):\n",
    "    word = word.lower()\n",
    "    if word.isalpha():\n",
    "        if sentiment:\n",
    "            if find_index(lexicon, word):\n",
    "                return word\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def tokenize_lyric2(lyric, sentiment=True):\n",
    "    ## Pre-procesamos el corpus\n",
    "    toktok = ToktokTokenizer()\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    words = toktok.tokenize(lyric)\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        stemmed = p_stemmer.stem(w)\n",
    "        clean_word = word_processor2(stemmed, sentiment, mixed_corpus)\n",
    "        if clean_word:\n",
    "            filtered_words.append(clean_word)\n",
    "            \n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "#  Tokenizamos y sacamos las letras con menos de 10 palabras\n",
    "tokenized_lyrics = []\n",
    "for lyric in lyrics_list:\n",
    "    tokenized = tokenize_lyric2(lyric, False)\n",
    "    if len(tokenized) >= 0:\n",
    "        tokenized_lyrics.append(tokenized)\n",
    "len(tokenized_lyrics)\n",
    "\n",
    "\n",
    "def get_context(word, corpus):\n",
    "    corpus_context = []\n",
    "    for lyric in corpus:\n",
    "        lyric_context = []\n",
    "        for i in range(len(lyric)):\n",
    "            if word == lyric[i]:\n",
    "                word_context = []\n",
    "                try:\n",
    "                    word_context.append(lyric[i-3])\n",
    "                    word_context.append(lyric[i-2])\n",
    "                    word_context.append(lyric[i-1])\n",
    "                    word_context.append(lyric[i])\n",
    "                    word_context.append(lyric[i+1])\n",
    "                    word_context.append(lyric[i+2])\n",
    "                    word_context.append(lyric[i+3])\n",
    "                    if not word_context in lyric_context:\n",
    "                        lyric_context.append(word_context)\n",
    "                except:\n",
    "                    pass\n",
    "        if lyric_context:\n",
    "            corpus_context.append(lyric_context)\n",
    "    return corpus_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.244*\"babi\" + 0.033*\"like\" + 0.032*\"rock\" + 0.031*\"parti\" + 0.028*\"time\" + 0.026*\"name\"')\n",
      "(1, '0.569*\"love\" + 0.025*\"like\" + 0.022*\"heart\" + 0.020*\"life\" + 0.011*\"time\" + 0.009*\"world\"')\n",
      "(2, '0.116*\"girl\" + 0.055*\"burn\" + 0.046*\"blue\" + 0.039*\"like\" + 0.030*\"dream\" + 0.026*\"free\"')\n",
      "(3, '0.129*\"christma\" + 0.096*\"war\" + 0.086*\"happi\" + 0.065*\"merri\" + 0.049*\"sin\" + 0.029*\"con\"')\n",
      "(4, '0.147*\"home\" + 0.063*\"joy\" + 0.059*\"good\" + 0.026*\"sweet\" + 0.023*\"honey\" + 0.023*\"sleep\"')\n",
      "(5, '0.076*\"god\" + 0.049*\"time\" + 0.046*\"fight\" + 0.030*\"life\" + 0.029*\"fall\" + 0.027*\"like\"')\n",
      "(6, '0.062*\"heart\" + 0.047*\"pain\" + 0.039*\"hate\" + 0.038*\"kiss\" + 0.024*\"better\" + 0.021*\"like\"')\n",
      "(7, '0.103*\"fire\" + 0.068*\"lone\" + 0.043*\"sun\" + 0.031*\"easi\" + 0.028*\"like\" + 0.027*\"lie\"')\n",
      "CPU times: user 1min 13s, sys: 8.39 s, total: 1min 22s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Resumen LDA\n",
    "num_topics = 8\n",
    "ldamodel = apply_lda(corpus_bow, num_topics, 100)\n",
    "print_lda(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster exploration from word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.0174636), (3, 0.005617701)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['never', 'ring', 'the', 'troubl', 'that', 'it', 'bring']],\n",
       " [['to', 'tell', 'my', 'troubl', 'to', 'i', 'don'],\n",
       "  ['to', 'tell', 'my', 'troubl', 'to', 'caus', 'sinc']],\n",
       " [['behind', 'me', 'where', 'troubl', 'melt', 'like', 'lemondrop']]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'troubl'\n",
    "print(ldamodel.get_term_topics(p_stemmer.stem(word)))\n",
    "\n",
    "res = get_context(word, tokenized_lyrics)\n",
    "res[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify new lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay_love = ''''\n",
    "Look at the stars\n",
    "Look how they shine for you\n",
    "And everything you do\n",
    "Yeah they were all yellow\n",
    "\n",
    "I came along\n",
    "I wrote a song for you\n",
    "And all the things you do\n",
    "And it was called yellow\n",
    "\n",
    "So then I took my turn\n",
    "Oh what a thing to have done\n",
    "And it was all yellow\n",
    "\n",
    "Your skin\n",
    "Oh yeah your skin and bones\n",
    "Turn into something beautiful\n",
    "You know you know I love you so\n",
    "You know I love you so\n",
    "\n",
    "I swam across\n",
    "I jumped across for you\n",
    "Oh what a thing to do\n",
    "\n",
    "'Cause you were all yellow\n",
    "I drew a line\n",
    "I drew a line for you\n",
    "Oh what a thing to do\n",
    "And it was all yellow\n",
    "\n",
    "Your skin\n",
    "Oh yeah your skin and bones\n",
    "Turn into something beautiful\n",
    "And you know\n",
    "For you I'd bleed myself dry\n",
    "For you I'd bleed myself dry\n",
    "\n",
    "It's true\n",
    "Look how they shine for you\n",
    "Look how they shine for you\n",
    "Look how they shine for\n",
    "Look how they shine for you\n",
    "Look how they shine for you\n",
    "Look how they shine\n",
    "\n",
    "Look at the stars\n",
    "Look how they shine for you\n",
    "And all the things that you do\n",
    "'''\n",
    "\n",
    "coldplay_sad = '''\n",
    "And the hardest part\n",
    "Was letting go, not taking part\n",
    "Was the hardest part\n",
    "\n",
    "And the strangest thing\n",
    "Was waiting for that bell to ring\n",
    "It was the strangest start\n",
    "\n",
    "I could feel it go down\n",
    "Bittersweet I could taste in my mouth\n",
    "Silver lining the cloud\n",
    "Oh and I\n",
    "I wish that I could work it out\n",
    "\n",
    "And the hardest part\n",
    "Was letting go, not taking part\n",
    "You really broke my heart, oh\n",
    "\n",
    "And I tried to sing\n",
    "But I couldn't think of anything\n",
    "And that was the hardest part, oh\n",
    "\n",
    "I could feel it go down\n",
    "You left the sweetest taste in my mouth\n",
    "You're silver lining the clouds\n",
    "Oh and I\n",
    "Oh and I\n",
    "I wonder what it's all about\n",
    "I wonder what it's all about\n",
    "\n",
    "Everything I know is wrong\n",
    "Everything I do, it's just comes undone\n",
    "And everything is torn apart\n",
    "\n",
    "Oh and it's the hardest part\n",
    "That's the hardest part\n",
    "Yeah that's the hardest part\n",
    "That's the hardest part\n",
    "'''\n",
    "\n",
    "eminem_violence = '''\n",
    "I told y'all mothafuckas I was comin' back \n",
    "What now nigga what now what \n",
    "You's the projects nigga\n",
    "One shot two shot three shot four shots \n",
    "All I hear is gunshots this is where the fun stops \n",
    "Bodies drop hit the floor music's off \n",
    "Parties stop, everybody hit the door someone's lickin' shots off\n",
    "You bitches is gone I'm dropped in the club \n",
    "And I'm tryna run and get my motherfuckin' gun \n",
    "(Nigga what about your wife) \n",
    "Nigga fuck my wife I'm tryna run and save my motherfuckin' life \n",
    "Oh shit the shoot is comin' \n",
    "Bitches, hoes niggas is runnin' \n",
    "People shot all over the floor \n",
    "And I'm tryna make it to the St. Andrew's door \n",
    "That's the sound of the glock \n",
    "Even D-J House fucked around and go shot \n",
    "I done messed around and forgot my tec \n",
    "I don't see nobody but Fab Five and Hex \n",
    "(Kuniva you aight) \n",
    "These niggas is trippin' \n",
    "(Where's Bizarre at?) \n",
    "I'm tryna slip through the exit and get to where my car is at \n",
    "Bitches screamin' everywhere and niggas is wildin' \n",
    "Two minutes ago we was all jokin' and smilin' \n",
    "This chick is clingin' onto me sobbin' and sighin' \n",
    "Sayin' she didn't mean to diss me earlier and she cryin' \n",
    "But its real and cats is gettin' killed \n",
    "So I hugged her and used her body as a human shield \n",
    "And she got hit now she yellin' \n",
    "(Don't leave me!) \n",
    "I told her I'd be right back and the dumb bitch believed me \n",
    "I squeezed through the back door and made my escape \n",
    "I ran and got my 38 I hope its not to late\n",
    "One shot two shot three shot four shots \n",
    "All I hear is gunshots this is where the fun stops \n",
    "Bodies drop hit the floor music's off \n",
    "Parties stop, everybody hit the door someone's lickin' shots off\n",
    "I'm on seven mile what the fuck was that \n",
    "Damn somebody hit me from the back \n",
    "(With they car?) \n",
    "With a gat nigga and my tire is flat \n",
    "And I just hit a pole, them niggas some hoes \n",
    "(Is you hit?) \n",
    "I don't know but I can tell you what they drove \n",
    "It was a black Mitsubishi \n",
    "(Shit that's the clique we beefin' wit I swear) \n",
    "Man and I was on my way there \n",
    "Believe me I'm leavin' a carcus today \n",
    "I'm a park my car and walk the rest of the way \n",
    "I'm in the mood to strut, my A-K ain't even tuck \n",
    "I'm a meet you at the club we goin' fuck these hoes up\n",
    "One shot two shot three shot four shots \n",
    "All I hear is gunshots this is where the fun stops \n",
    "Bodies drop hit the floor music's off \n",
    "Parties stop, everybody hit the door someone's lickin' shots off\n",
    "I never seen no shit like this is my life before \n",
    "People will still camp out from the night before \n",
    "Sleepin' outside the door waitin' in line \n",
    "Still tryna get inside the club to see D12 perform \n",
    "The fire marshals no, the venue's too small \n",
    "People are wall to wall three thousand and some odd vans \n",
    "And some come walk from out the parkin' lot \n",
    "Get into an argument over a parkin' spot \n",
    "He's about to pull his gun out and let's a few of 'em off \n",
    "Missed who he's aimin' for six feet away's the door \n",
    "In St. Andrew's hall not a stray slidin' all over the place \n",
    "Sprays one bitch in the face another one of 'em came through the wall \n",
    "Before anyone could even hear the first shot go off \n",
    "I'm posted up by the bar havin' a Mozeltoff \n",
    "Bullet wizzed right by my ear damn near shot it off \n",
    "Thank god I'm alive I gotta find Denaun \n",
    "And where the fuck is Von he usually tucks one on him \n",
    "Wait a minute I think I just saw Bizarre \n",
    "Nah I guess not, what the fuck oh my god it was \n",
    "I never saw him run so fast in my life \n",
    "Look at him haulin' ass I think he left his wife \n",
    "There she is on the ground bein' trampled \n",
    "I go to grab her up by the damn hand and I can't pull her \n",
    "God damn there just went another damn bullet I'm hit \n",
    "My vest is barely able to handle it, its to thin \n",
    "If I get hit again I can't do it, I scoop deep \n",
    "Follow Bizarre's path and ran through it \n",
    "And made it to the front door and collapsed on the steps \n",
    "Looked up and I seen Swift shootin' it out \n",
    "But I can't see who he's shootin' it out with \n",
    "But Denaun's right behind him squeezin' his four fifth\n",
    "One shot two shot three shot four shots \n",
    "All I hear is gunshots this is where the fun stops \n",
    "Bodies drop hit the floor music's off \n",
    "Parties stop, everybody hit the door someone's lickin' shots off\n",
    "It's Friday night came to this bitch right \n",
    "Big ass to my left and Desert Eagle to my right \n",
    "I ain't come in this bitch to party I came in this bitch to fight \n",
    "Although I can't stay here to fight 'cause I'm poppin' niggas tonight \n",
    "That's right bitches I'm drunk with revenge \n",
    "Shot a bouncer in the neck for tryna check when I get in \n",
    "Swift told me to meet him here so its clear that this fucker\n",
    "Shoot out the back of his truck goes up in this motherfucker \n",
    "So one shot for the money two's to stop the show \n",
    "Third's for the bartender there's plenty of shots to go \n",
    "(I just wanna know who's drivin' a black Mitsubishi) \n",
    "He tried to run so Proof shot him in the knee wit a three piece\n",
    "One shot two shot three shot four shots \n",
    "All I hear is gunshots this is where the fun stops \n",
    "Bodies drop hit the floor music's off \n",
    "Parties stop, everybody hit the door someone's lickin' shots off\n",
    "'''\n",
    "\n",
    "depressing_sad = '''\n",
    "I was bruised and battered, I couldn't tell what I felt.\n",
    "I was unrecognizable to myself.\n",
    "Saw my reflection in a window and didn't know my own face.\n",
    "Oh brother are you gonna leave me wastin' away\n",
    "On the streets of Philadelphia.\n",
    "\n",
    "I walked the avenue, 'til my legs felt like stone, \n",
    "I heard the voices of friends, vanished and gone, \n",
    "At night I could hear the blood in my veins, \n",
    "It was just as black and whispering as the rain, \n",
    "On the streets of Philadelphia.\n",
    "\n",
    "Ain't no angel gonna greet me.\n",
    "It's just you and I my friend.\n",
    "And my clothes don't fit me no more, \n",
    "I walked a thousand miles\n",
    "Just to slip this skin.\n",
    "\n",
    "Night has fallen, I'm lyin' awake, \n",
    "I can feel myself fading away, \n",
    "So receive me brother with your faithless kiss, \n",
    "Or will we leave each other alone like this\n",
    "On the streets of Philadelphia.\n",
    "'''\n",
    "\n",
    "love_happy = '''\n",
    "Imagine me and you, I do\n",
    "I think about you day and night, it's only right\n",
    "To think about the girl you love and hold her tight\n",
    "So happy together\n",
    "If I should call you up, invest a dime\n",
    "And you say you belong to me and ease my mind\n",
    "Imagine how the world could be, so very fine\n",
    "So happy together\n",
    "I can't see me lovin' nobody but you\n",
    "For all my life\n",
    "When you're with me, baby the skies'll be blue\n",
    "For all my life\n",
    "Me and you and you and me\n",
    "No matter how they toss the dice, it had to be\n",
    "The only one for me is you, and you for me\n",
    "So happy together\n",
    "I can't see me lovin' nobody but you\n",
    "For all my life\n",
    "When you're with me, baby the skies'll be blue\n",
    "For all my life\n",
    "Me and you and you and me\n",
    "No matter how they toss the dice, it had to be\n",
    "The only one for me is you, and you for me\n",
    "So happy together\n",
    "Ba-ba-ba-ba ba-ba-ba-ba ba-ba-ba ba-ba-ba-ba\n",
    "Ba-ba-ba-ba ba-ba-ba-ba ba-ba-ba ba-ba-ba-ba\n",
    "Me and you and you and me\n",
    "No matter how they toss the dice, it had to be\n",
    "The only one for me is you, and you for me\n",
    "So happy together\n",
    "So happy together\n",
    "How is the weather\n",
    "So happy together\n",
    "We're happy together\n",
    "So happy together\n",
    "Happy together\n",
    "So happy together\n",
    "So happy together (ba-ba-ba-ba ba-ba-ba-ba)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow - Love\n",
      "Sentiment Words\n",
      "{'star', 'yellow', 'song', 'shine', 'bleed', 'beauti', 'love'}\n",
      "[(0, 0.26569563), (1, 0.14224355), (2, 0.005443275), (3, 0.0054450766), (4, 0.0054396936), (5, 0.0054365839), (6, 0.0054387404), (7, 0.56485742)]\n",
      "Hardest part - Love Sad\n",
      "Sentiment Words\n",
      "{'part', 'wrong', 'undon', 'heart', 'strangest', 'wonder', 'tast', 'work', 'cloud', 'broke'}\n",
      "[(0, 0.0052139075), (1, 0.0052115009), (2, 0.0052127019), (3, 0.0052145175), (4, 0.0052107354), (5, 0.0052111535), (6, 0.69215435), (7, 0.27657118)]\n",
      "Street of philadelphia - Depressing, sad\n",
      "Sentiment Words\n",
      "{'fallen', 'friend', 'avenu', 'black', 'angel', 'faithless', 'rain', 'batter', 'bruis', 'like', 'kiss', 'brother', 'greet', 'cloth', 'window', 'alon', 'street'}\n",
      "[(0, 0.005694435), (1, 0.19262962), (2, 0.28156328), (3, 0.0056924908), (4, 0.10997805), (5, 0.005693973), (6, 0.23129344), (7, 0.16745467)]\n",
      "Eminem | One shot, two shot - Violence, Anger\n",
      "Sentiment Words\n",
      "{'gun', 'right', 'human', 'hit', 'black', 'door', 'argument', 'clear', 'kill', 'like', 'bizarr', 'desert', 'bar', 'peopl', 'mess', 'plenti', 'fight', 'shit', 'vest', 'strut', 'damn', 'parti', 'fun', 'hope', 'drunk', 'reveng', 'fire', 'fast', 'face', 'hand', 'floor', 'odd', 'man', 'aliv', 'life', 'sound', 'outsid', 'god', 'thank', 'save', 'swift', 'truck', 'miss', 'wife', 'bitch', 'tire', 'use', 'collaps', 'bodi', 'hous', 'believ', 'music', 'dumb', 'cliqu', 'diss', 'spray', 'fuck', 'hug', 'bullet', 'money', 'cat', 'car'}\n",
      "[(0, 0.36091372), (2, 0.27651325), (3, 0.034427702), (4, 0.015699007), (5, 0.24461746), (6, 0.013248404), (7, 0.053729638)]\n",
      "Bob Marley | Three Little Birds - Love Happy\n",
      "Sentiment Words\n",
      "{'right', 'love', 'eas', 'life', 'imagin', 'invest', 'sky', 'babi', 'blue', 'world', 'mind', 'girl', 'fine', 'happi'}\n",
      "[(0, 0.092307277), (1, 0.092193618), (2, 0.43566039), (3, 0.36466655), (4, 0.0037897446), (5, 0.0037954003), (6, 0.003794159), (7, 0.0037928133)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Love\n",
    "new_tokenized = tokenize_lyric(coldplay_love)\n",
    "vec = dictionary.doc2bow(new_tokenized)\n",
    "topics_list = ldamodel[vec]\n",
    "print(\"Yellow - Love\")\n",
    "print(\"Sentiment Words\")\n",
    "print(set(new_tokenized))\n",
    "print(topics_list)\n",
    "print\n",
    "\n",
    "# Love Melancholic, sad\n",
    "new_tokenized = tokenize_lyric(coldplay_sad)\n",
    "vec = dictionary.doc2bow(new_tokenized)\n",
    "topics_list = ldamodel[vec]\n",
    "print(\"Hardest part - Love Sad\")\n",
    "print(\"Sentiment Words\")\n",
    "print(set(new_tokenized))\n",
    "print(topics_list)\n",
    "print\n",
    "\n",
    "# Depressing, sad\n",
    "new_tokenized = tokenize_lyric(depressing_sad)\n",
    "vec = dictionary.doc2bow(new_tokenized)\n",
    "topics_list = ldamodel[vec]\n",
    "print(\"Street of philadelphia - Depressing, sad\")\n",
    "print(\"Sentiment Words\")\n",
    "print(set(new_tokenized))\n",
    "print(topics_list)\n",
    "print\n",
    "\n",
    "# Violence, Anger\n",
    "new_tokenized = tokenize_lyric(eminem_violence)\n",
    "vec = dictionary.doc2bow(new_tokenized)\n",
    "topics_list = ldamodel[vec]\n",
    "print(\"Eminem | One shot, two shot - Violence, Anger\")\n",
    "print(\"Sentiment Words\")\n",
    "print(set(new_tokenized))\n",
    "print(topics_list)\n",
    "print\n",
    "\n",
    "\n",
    "# Happy\n",
    "new_tokenized = tokenize_lyric(love_happy)\n",
    "vec = dictionary.doc2bow(new_tokenized)\n",
    "topics_list = ldamodel[vec]\n",
    "print(\"Bob Marley | Three Little Birds - Love Happy\")\n",
    "print(\"Sentiment Words\")\n",
    "print(set(new_tokenized))\n",
    "print(topics_list)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
